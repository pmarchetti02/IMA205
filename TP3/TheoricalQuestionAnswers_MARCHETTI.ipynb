{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMA205 - Supervised Learning - MARCHETTI Pierre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorical questions\n",
    "\n",
    "## OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First we need to compute $\\mathbb{E}[\\tilde{\\beta}]$ : \n",
    "$$\\mathbb{E}[\\tilde{\\beta}]= \\mathbb{E}[CY]=CX\\beta + C\\mathbb{E}[\\epsilon]=HX\\beta + DX\\beta$$ \n",
    "\n",
    "Since $\\tilde{\\beta}$ is ubsiased, $DX\\beta=0, \\forall{\\beta}$ and then $DX=0$.\n",
    "\n",
    "- Now we need to compute $Var(\\tilde{\\beta})$ : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{Var}(\\tilde{\\beta}) &= Var(CY) \\\\\n",
    "                  &= \\sigma^2CC^T \\\\\n",
    "                  &= \\sigma^2(H+D)(H^T+D^T) \\\\\n",
    "                  &= \\sigma^2HH^T+\\sigma^2DD^T+\\sigma^2HD^T+\\sigma^2DH^T \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "But, $$DH^T=(DX)(X^TX)^{-1}=0$$\n",
    "\n",
    "$$H^TD=(DH^T)^T=0^T=0$$\n",
    "\n",
    "$$Var(\\beta^*)=\\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}=\\sigma^2HH^T$$\n",
    "\n",
    "Then, $$\\boxed{Var(\\tilde{\\beta})=\\text{Var}(\\beta^*)+\\sigma^2DD^T}$$\n",
    "\n",
    "Thus, $Var(\\tilde{\\beta})-Var(\\beta^*)=\\sigma^2DD^T$\n",
    "\n",
    "But, $\\forall z \\in \\mathbb{R}^d \\setminus \\{0\\}, \\quad z^TDD^Tz = \\left\\| \\mathbf{D^Tz} \\right\\| \\gt 0$ because $D \\neq{0}$\n",
    "\n",
    "Since, $$\\boxed{Var\\tilde{\\beta} \\gt Var(\\beta^*)}$$\n",
    "\n",
    "Here, we supposed a gaussian model such that $(X^TX)$ is invertible.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have to show that the estimator of ridge regression is biased.\n",
    "\n",
    "The ridge solution can be written as $\\beta_{ridge}^*=(x_c^Tx_c+\\lambda I)^{-1}x_c^Ty_c$\n",
    "\n",
    "Then we have, $\\mathbb{E}[\\beta_{ridge}^*]=\\mathbb{E}[(x_c^Tx_c+\\lambda I)^{-1}x_c^Ty_c]=[(x_c^Tx_c+\\lambda I)^{-1}x_c^T]\\mathbb{E}[y_c]$ \n",
    "\n",
    "\n",
    "$$ \\mathbb{E}[\\beta_{ridge}^*]=[(x_c^Tx_c+\\lambda I)^{-1}x_c^Tx_c]\\beta \\neq{\\beta} \\text{ unless } \\lambda=0$$ \n",
    "\n",
    "Thus, the estimator is **biased**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have to recall the SVD decomposition and write the solution using the SVD.\n",
    "\n",
    "We have,  $\\text{  } x_c^Tx_c = VDU^TUDV^T = VD^TDV^T$ because U is orthogonal.\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\beta_{\\text{ridge}}^* &= (VD^TDV^T + \\lambda VV^T)^{-1} x_c^T y_c \\text{ because } V \\text{ is orthogonal} \\\\\n",
    "    &= V(D^TDV^T+\\lambda VV^T)^{-1}x_c^Ty_c \\\\\n",
    "    &= V\\text{diag}(\\mu_i^2+\\lambda)^{-1}V^Tx_c^Ty_c \\\\\n",
    "    &= V\\text{diag}(\\frac{1}{\\mu_i^2+\\lambda})V^Tx_c^Ty_c \\\\\n",
    "\\end{align*}\n",
    "\n",
    "$$\n",
    "\n",
    "It is useful to use this decomposition to invert a complex matrix. Instead, we just have to invert the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have to show that $Var(\\beta_{OLS}^*) \\geq Var(\\beta_{ridge}^*)$\n",
    "\n",
    "On one hand we have, \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    Var(\\beta_{\\text{ridge}}^*) &= \\sigma^2V(D^TD+\\lambda I_d)^{-1}V^Tx_c^Tx_cV(D^TD+\\lambda I_d)^{-1}V^T \\\\\n",
    "    &= \\sigma^2V(D^TD+\\lambda I_d)^{-1}D^TD(D^TD+\\lambda I_d)^{-1}V^T \\\\\n",
    "    &= \\sigma^2V\\text{diag}(\\frac{1}{\\mu_i^2+\\lambda})\\text{diag}(\\mu_i^2)\\text{diag}(\\frac{1}{\\mu_i^2+\\lambda})V^T \\\\\n",
    "    &= \\sigma^2V\\text{diag}(\\frac{\\mu_i^2}{(\\mu_i^2+\\lambda)^2})V^T \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "On the other hand we have, \n",
    "$$\n",
    "\\begin{align*}\n",
    "    Var(\\beta_{OLS}^*)=\\sigma^2V\\text{diag}(\\frac{1}{\\mu_i^2})V^T \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, $$Var(\\beta_{OLS}^*)-Var(\\beta_{\\text{ridge}}^*)=\\sigma^2V\\text{diag}(\\frac{1}{\\mu_i^2}-\\frac{\\mu_i^2}{(\\mu_i^2+\\lambda)^2})V^T$$\n",
    "\n",
    "But, $\\forall{i}, \\frac{1}{\\mu_i^2}-\\frac{1}{\\mu_i^2}-\\frac{\\mu_i^2}{(\\mu_i^2+\\lambda)^2} \\geq 0$\n",
    "\n",
    "We then have, $$Var(\\beta_{OLS}^*) \\geq Var(\\beta_{ridge}^*)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We've seen that, $Var(\\beta_{\\text{ridge}}^*) = \\sigma^2V\\text{diag}(\\frac{\\mu_i^2}{(\\mu_i^2+\\lambda)^2})V^T$, thus if $\\lambda$ increase then this quantity become smaller. Conversely, if $\\lambda$ increase then the bias increase too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally we have to show that $\\beta_{ridge}^*=\\frac{\\beta_{OLS}^*}{1+\\lambda} \\text{ when } x_c^Tx_c=I_d$\n",
    "\n",
    "We have, $$\\beta_{ridge}^*=(x_c^Tx_c+\\lambda I_d)^{-1}x_c^Ty_c=((1+\\lambda)I_d)^{-1}x_c^Ty_c$$\n",
    "\n",
    "We also have, $$\\beta_{OLS}^*=(x_c^Tx_c)^{-1}x_c^Ty_c=x_c^Ty_c$$\n",
    "\n",
    "Then it is direct that $$\\beta_{ridge}^*=\\frac{\\beta_{OLS}^*}{1+\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have to compute the solution of the equation : \n",
    "\n",
    "$$\\beta_{ElNet}^* = \\mathrm{argmin}_{\\beta}(y_c-x_c\\beta)^T(y_c-x_c\\beta)+\\lambda_2 \\left\\| \\mathbf{\\beta} \\right\\|_2^2 + \\lambda_1 \\left\\| \\mathbf{\\beta} \\right\\|_1$$\n",
    "\n",
    "We assume $X^TX=I_d$\n",
    "\n",
    "Denoting $f$ the objective function we have : \n",
    "\n",
    "$$f(\\beta)=\\left\\| \\mathbf{Y-X\\beta} \\right\\|^2 + \\lambda_2 \\left\\| \\mathbf{\\beta} \\right\\|_2^2 + \\lambda_1 \\left\\| \\mathbf{\\beta} \\right\\|_1$$\n",
    "\n",
    "Then, \n",
    "\n",
    "$$\\nabla_{\\beta}f(\\beta) = -2X^T(Y-X\\beta)+2\\lambda_2\\beta+\\nabla_{\\beta}(\\lambda_1 \\left\\| \\mathbf{\\beta} \\right\\|_1)$$\n",
    "\n",
    "But, $\\left\\| \\mathbf{\\beta} \\right\\|_1 = \\sum_{i=1}^{n}{|\\beta_i|}$ so $\\nabla_{\\beta}(\\lambda_1 \\left\\| \\mathbf{\\beta} \\right\\|_1) = \\pm \\lambda_1$\n",
    "\n",
    "Then, \n",
    "\n",
    "$$\\nabla_{\\beta}f(\\beta) = -2X^T(Y-X\\beta)+2\\lambda_2\\beta \\pm \\lambda_1 \\nabla_{\\beta}f(\\beta)$$\n",
    "\n",
    "That being said, \n",
    "\n",
    "$$\\nabla_{\\beta}f(\\beta)=0 \\leftrightarrow\n",
    " 2X^TY \\pm \\lambda_1 = 2(1+\\lambda_2)\\beta$$\n",
    "\n",
    " And finally, $$\\boxed{\\beta_{ElNet}^* = \\frac{\\beta_{OLS}^* \\pm \\frac{\\lambda_1}{2}}{1+\\lambda_2}}$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
